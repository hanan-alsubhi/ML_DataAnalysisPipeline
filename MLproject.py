# -*- coding: utf-8 -*-
"""MLproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17K00altm87WAjDUXfdYvRYH-bhAAgGSR

#**Frame Work And Tools Project**
###By :

####**Alhanouf Fawaz Aldossari**

ID: 444004951

####**Hanan adnan al-subhi**

ID:444006546

####**Maya Abudlraheem Tayeb**

ID:444002353

####**Sarah Mohammed Althobaiti**

ID:444004881

import
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

"""# **phase 1: Data Wrangling and Exploration with Pandas**

**2.1 Dataset Overview :**
"""

# Load the dataset
df = pd.read_csv('/content/diabetes.csv')

# Display the number of rows and columns
print(f"Number of (rows, columns) = {df.shape}")

# Preview the first 5 rows of the dataset
print("\nDataset Preview:")
display(df.head())

# Display information about the DataFrame
print("\nInformation about DataFrame:")
df.info()

print("\nData types of each column in the dataset:")
display(df.dtypes)

print("\nlast 5 rows:")
display(df.tail())

"""The dataset was loaded using the pd.read_csv() function, and it contains 768 rows and 9 columns. A preview of the first five rows was displayed, showing various features like Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age, and Outcome.

The dataset contains no missing values, as all columns have 768 non-null entries. The columns are of the following data types:

int64 for most of the columns: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, Age, and Outcome.
float64 for BMI and DiabetesPedigreeFunction.
The datasetâ€™s memory usage is 54.1 KB, and the information about the data types and non-null counts confirms that the dataset is complete with no missing or invalid data types.

**2.2 Data cleaning :**
"""

# Check for null values
null_counts = df.isnull().sum()
print(f"Check if there are null values in the dataset:\n{null_counts}")

# Check for duplicate rows
print("\nChecking for duplicate rows in the dataset:")
duplicate_rows = df[df.duplicated()]

# Display results
if not duplicate_rows.empty:
    print(f"Duplicate rows found:\n{duplicate_rows}")
else:
    print("No duplicate rows found in the dataset.")

"""The analysis revealed that there are no null values in the dataset, as all columns showed a count of zero for missing values. This was confirmed by checking each column for null entries using the isnull() method. Additionally, a check for duplicate rows was performed, and it was determined that there were no duplicate rows in the dataset, as the duplicated() method returned an empty result.

**2.2 Data cleaning :**
"""

# Calculate the first quartile (Q1) and third quartile (Q3) for the numerical data
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)

# Calculate the interquartile range (IQR)
IQR = Q3 - Q1

# Define the lower and upper bounds for outlier detection
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter the DataFrame to exclude rows with values outside the IQR bounds
filtered_df = df[~((df < lower_bound) | (df > upper_bound)).any(axis=1)]

# Print the shape of the filtered DataFrame (before any removal)
print("Shape before filtered DataFrame:", df.shape)

# Print the shape of the filtered DataFrame (after removing outliers)
print("Shape after filtered DataFrame:", filtered_df.shape)

# Print the number of removed rows
print("Number of rows removed:", len(df) - len(filtered_df))

# Plot the boxplot for the original DataFrame (with potential outliers)
plt.figure(figsize=(15, 10))
sns.boxplot(data=df)
plt.title("Boxplots for Numerical Features")
plt.xticks(rotation=45)
plt.show()

# Plot the boxplot for the filtered DataFrame (with outliers removed)
plt.figure(figsize=(15, 10))
sns.boxplot(data=filtered_df)
plt.title("Boxplots for Numerical Features")
plt.xticks(rotation=45)
plt.show()

"""Before filtering, the dataset contained many outliers, as seen in the first boxplot, which showed significant inconsistencies in the data distribution. The original DataFrame had a shape of (768, 9). After applying the outlier removal process, 129 rows were removed, resulting in a filtered DataFrame with a shape of (639, 9). The second boxplot, after the removal of outliers, displays a much more consistent distribution of numerical features, suggesting that the data is now cleaner and more reliable for further analysis.

**2.3 Exploratory Data Analysis (EDA)**
"""

# Load dataset
filted_df = pd.read_csv('diabetes.csv')

# Summary statistics with Median ans Variance
print("Statistics Summary\n")
display(filted_df.describe())

print("\nMedian:")
display(filted_df.median())

print("\nVariance:")
display(filted_df.var())

# Histograms
num_columns = len(filted_df.columns)
num_rows = (num_columns // 3) + (num_columns % 3 > 0)  #3 columns per row

fig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(15, 5 * num_rows))
axes = axes.flatten()  # Flatten axes to simplify indexing

# Plot each column
for i, column in enumerate(filted_df.columns):
    if filted_df[column].dtype != 'object':  # Only for numerical columns
        sns.histplot(data=filted_df, x=column, kde=True, bins=20, ax=axes[i])
        axes[i].set_title(f"{column} Histogram", fontsize=10)  # Adjust title font size
    else:
        axes[i].remove()  # Remove axes for non-numeric columns

# Remove any unused axes
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

# Adjust layout and spacing
fig.subplots_adjust(hspace=0.5, wspace=0.3)  # Adjust vertical (hspace) and horizontal (wspace) space
plt.suptitle("Histograms for Each Column", fontsize=14, y=1.02)  # Adjust main title position
plt.show()

"""1- The statistical summary complements the visualizations, helping confirm data trends and potential preprocessing steps, such as handling missing values, normalizing skewed data, and addressing class imbalance.

- The Median Provides the median (middle value) for each numerical column, offering insights into central tendencies.

- Variance Displays the variance for each column. Larger variance indicates more significant spread in the data.

2-The histograms provide a clear visualization of data distribution, potential missing data.
Each histogram represents the distribution of a specific column in the dataset, helping identify patterns such as skewness, modality, or the presence of outliers.
"""

plt.figure(figsize=(10, 6))
sns.heatmap(filted_df.corr(), annot=True, cmap="BuPu", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

"""
The features with higher correlation to the target variable (Outcome) might be important predictors for diabetes."""

# 1 indicates that the patient has diabetes, and 0 indicates they do not.
sns.countplot(x='Outcome', data=filted_df)
plt.title('Distribution of Diabetes Outcome')
plt.show()

"""A significantly larger number of individuals in the dataset are non-diabetic (Outcome = 0) compared to diabetic individuals (Outcome = 1).
The class imbalance suggests that the dataset contains more non-diabetic cases than diabetic cases, which is common in real-world medical datasets.

**2.4 Data Manipulation**
"""

# Create two separate DataFrames from 'Age' and 'BMI'
age_df = filted_df[['Age']].reset_index()  # Extract 'Age' and reset the index
bmi_df = filted_df[['BMI']].reset_index()  # Extract 'BMI' and reset the index

# Merge the two DataFrames on the 'index' column
merged_df = pd.merge(age_df, bmi_df, on='index')

#Add a new column that captures the interaction between Age and BMI
merged_df['BMI_Age_Trend'] = merged_df['Age'] * merged_df['BMI']

#Merge this new feature back into the original DataFrame
final_df = pd.merge(filted_df, merged_df[['index', 'BMI_Age_Trend']], left_index=True, right_on='index').drop(columns=['index'])

# Display the first five rows of the final DataFrame
display(final_df.head())

# Filter rows where Glucose is greater than 140 (indicative of high risk)
high_glucose = filted_df[filted_df['Glucose'] > 140]
print("\nHigh Glucose Levels")
display(high_glucose.head())

"""We performs feature engineering by creating a new feature (BMI_Age_Trend) to capture the interaction between Age and BMI. We also filters the dataset to analyze individuals with high glucose levels.

1. Merging Age and BMI to Create a New Feature (BMI_Age_Trend)

- two separate DataFrames (age_df and bmi_df) are created, each
containing one of the original columns (Age and BMI).
- The merge function is used to combine the two DataFrames based on their index.
- A new column (BMI_Age_Trend) is created by multiplying Age and BMI to capture their interaction.
- The new feature is merged back into the original dataset (final_df) and displayed.


2. Filtering Individuals with High Glucose Levels (Glucose > 140)
- The dataset is filtered to select rows where the Glucose value is greater than 140, indicating a high risk of diabetes.
- The display(high_glucose.head()) function is used to show the first five rows of the filtered dataset.

# **Phase 2: Numerical Computation with NumPy**
"""

# converts a Pandas DataFrame into a NumPy array for easier math calculations.
data = filtered_df.to_numpy()

"""part 1:
Perform vectorized operations and array manipulations for performance optimization.
"""

# Ensure filtered_df is a copy of the original DataFrame to avoid the warning
# square the BMI values and store them in a new column.
filtered_df = filtered_df.copy()
bmi_squared = data[:, 5] ** 2
# Use .loc[] to explicitly modify the DataFrame and avoid SettingWithCopyWarning
filtered_df.loc[:, "BMI_Squared"] = bmi_squared
print(filtered_df[['BMI', 'BMI_Squared']].head())

# filtered version of the Glucose column, where Glucose values over 120.
glucose = data[:, 1]
filtered_glucose = np.where(data[:, 1] > 120, data[:, 1], np.nan)
filtered_df.loc[:, "filtered_glucose"] = filtered_glucose
print("DataFrame with Filtered Glucose Column:")
print(filtered_df[['Glucose', 'filtered_glucose']].head())

"""We effectively utilized NumPy to perform numerical computations, emphasizing performance optimization through vectorized operations and array manipulations.

part 2: Calculate statistical measures such as mean, median, variance, and correlation using NumPy.
"""

Age = data[:, 7]

## Calculate the mean, median, and variance of Age
print(f"Mean Age: {np.mean(Age)}")
print(f"Median Age: {np.median(Age)}")
print(f"Variance Age: {np.var(Age)}")

## compute the correlation between Age and BloodPressure.
BloodPressure = data[:, 2]
correlation = np.corrcoef(Age, BloodPressure)[0, 1]
print(f"Correlation between Age and BloodPressure: {correlation}")

"""Statistical measures such as mean, median, variance, and correlation were calculated to extract valuable insights from the dataset, enhancing our understanding of the relationships between variables.

part 3: Implement custom algorithms for matrix operations to solidify understanding of numerical computing principles.
"""

## first algorithm that Calculate the determinant of the matrix:
def determinant(A):
    if len(A) == 2:
        return A[0][0] * A[1][1] - A[0][1] * A[1][0]
    return sum((-1) ** c * A[0][c] * determinant(minor(A, 0, c)) for c in range(len(A)))

def minor(A, i, j):
    return [row[:j] + row[j+1:] for row in (A[:i] + A[i+1:])]

# matrix contain Age and BloodPressure
matrix = data[:2, [7, 2]]
print("Determinant of the Matrix:", determinant(matrix))

## Calculate the determinant of the matrix using NumPy
det_A = np.linalg.det(matrix)
print("Determinant using NumPy:", det_A)

## second algorithm that Calculate the multiplication of the matrix:
def matrix_multiplication(A, B):
    if len(A[0]) != len(B):
        raise ValueError("Number of columns in A must equal number of rows in B.")
    return [[sum(A[i][k] * B[k][j] for k in range(len(B))) for j in range(len(B[0]))] for i in range(len(A))]

A = matrix
B = matrix.T
result = matrix_multiplication(A.tolist(), B.tolist())

# The result is printed for the first 5 rows and columns for large matrices to save on computation time.
print("Matrix Multiplication Result (Sample):")
for row in result[:5]:
    print(row[:5])

# Matrix multiplication using NumPy
# The result also printed for the first 5 rows and columns for large matrices to save on computation time.
print("Matrix Multiplication using NumPy (Sample):")
result = np.dot(A, B)
print(result[:5, :5])

## third algorithm that Calculate the frobenius norm of the matrix:

## first we Extract the Glucose column as a NumPy array
Glucose = filtered_df["Glucose"].to_numpy()
BMI = filtered_df["BMI"].to_numpy()

# Combine the first two rows of Glucose and BMI into a new matrix using column_stack.
matrix2 = np.column_stack((Glucose[:2], BMI[:2]))

# Function to calculate the Frobenius norm manually by summing the squares of all elements in the matrix.
def frobenius_norm(matrix2):
    sum_of_squares = 0
    for row in matrix:
        for element in row:
            sum_of_squares += element ** 2
    return sum_of_squares ** 0.5

print("Frobenius Norm:", frobenius_norm(matrix))

# Function to calculate the Frobenius norm using NumPy
def frobenius_norm_numpy(matrix2):
    return np.linalg.norm(matrix, 'fro')

frobenius_norm_value = frobenius_norm_numpy(matrix2)
print("Frobenius Norm (NumPy):", frobenius_norm_value)

"""Additionally, implementing custom algorithms for matrix operations, such as multiplication, determinants, and norms, reinforced our grasp of numerical computing principles and provided deeper insights into the dataset.

This combination of techniques highlights the power and flexibility of NumPy for efficient data analysis and numerical problem-solving.

# **phase 3: Machine Learning with Scikit-learn**

**splitting datasets** tool in Scikit-learn's that splits
the dataset into training and test sets.
"""

X = filtered_df.iloc[:, 0:8]  #all columns
y = filtered_df['Outcome']

# Train Test Split,# Divide the dataset into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**Naive Bayes model**"""

# bulid model

# Initialize the Naive Bayes model
model = GaussianNB()

# Fit the model using the training data
model.fit(X_train, y_train)

#Uses the trained model to predict the outcomes for the testing data (X_test).
y_pred = model.predict(X_test)

# Evaluate model
# y_test is the actual labels of testing data (X_test) to compare it with y_pred
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)

scores = cross_val_score(model, X, y, cv=5)
print(f"Cross-validation scores: {scores}")

conf_matrix = confusion_matrix(y_test, y_pred)
cmd = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['No Diabetes', 'Diabetes'])

plt.figure(figsize=(8, 6))
cmd.plot(cmap='PuBu')
plt.title('Confusion Matrix')
plt.show()

"""An accuracy of 78.91% for a Naive Bayes model means that the model correctly predicted the outcomes of the test data approximately 79%.

**Now we apply Random Frorest model**
"""

# Initialize the Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the Random Forest model using the training data. So, the model learns patterns in the data
model.fit(X_train, y_train)

# Use the trained Random Forest model to predict the target labels for the test data (X_test).
y_pred = model.predict(X_test)

# Evaluate model
# y_test is the actual labels of testing data (X_test) to compare it with y_pred
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)

scores = cross_val_score(model, X, y, cv=5)
print(f"Cross-validation scores: {scores}")

conf_matrix = confusion_matrix(y_test, y_pred)
cmd = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['No Diabetes', 'Diabetes'])

plt.figure(figsize=(8, 6))
cmd.plot(cmap='Purples')
plt.title('Confusion Matrix')
plt.show()

"""The model achieved an accuracy of **80.47%**, indicating that it correctly classified approximately 80% of the test cases. This is a solid performance, suggesting that the Random Forest Classifier is effective at distinguishing between the two categories ("No Diabetes" and "Diabetes") in the dataset."""